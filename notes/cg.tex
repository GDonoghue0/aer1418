\chapter{Conjugate gradient method}

\section{Introduction}
As we have seen in the previous lectures, a finite element approximation of a PDE requires the solution $x \in \RR^n$ to a linear system of the form
\begin{equation}
  Ax = b,
  \label{eq:cg_prob}
\end{equation}
where $A$ is an $n \times n$ matrix and $b$ is an $n$-vector.  In addition, we have seen that the matrix $A$ has rather special properties. First, for a finite element method based on basis functions with a compact support, the matrix $A$ is \emph{sparse}. We recall that a matrix is said to be sparse if most of its entries are zero, and the number of non-zero entries is $\calO(n)$ (as opposed to $n^2$ for a dense matrix). Second, if the underlying PDE is symmetric and coercive, then the matrix $A$ is symmetric positive definite (SPD).  We recall that a matrix is said to be symmetric positive definite if it is (i) symmetric so that $A = A^T$ and (ii) positive definite so that $x^T A x > 0$ for all $x \neq 0$ (or, equivalently, all eigenvalues are positive).  We can in fact leverage these special properties to more rapidly find the solution to \eqref{eq:cg_prob} than say Gaussian elimination.  Specifically, in this lecture, we introduce the \emph{conjugate gradient method}, which is arguably the most common approach to solve large, sparse, SPD linear system.  


\section{The conjugate gradient method}
We first introduce the conjugate gradient method in Algorithm~\ref{alg:cg}.
\begin{algorithm}
  \caption{Conjugate gradient method. \label{alg:cg}}

  $x_0 =0, \ r_0 = b, \ d_0 = r_0$ \hfill (initialization) \\
  \For{$i = 1,\dots$}{
    $\displaystyle \alpha_i = \frac{r^T_{i-1}r_{i-1}}{d_{i-1}^T A d_{i-1}}$
    \hfill (step length) \\
    $x_i = x_{i-1} + \alpha_i d_{i-1}$
    \hfill (solution update) \\
    $r_i = r_{i-1} - \alpha_i A d_{i-1}$
    \hfill (residual update) \\
    $\displaystyle \beta_i = \frac{r_i^T r_i}{r_{i-1}^T r_{i-1}}$
    \hfill ($A$-orthgonalization) \\
    $d_i = r_i + \beta_i d_{i-1}$
    \hfill (search direction)
  }
\end{algorithm}

\section{Krylov subspace, orthogonal residual, and $A$-orthogonal search directions}
In order to analyze the conjugate gradient algorithm, we first introduce the \emph{Krylov subspaces}.  The Krylov subspaces associated with a matrix $A$ and a vector $b$ is a series of subspaces given by
\begin{equation*}
  \calK_i \equiv \text{span} \{ b, Ab, A^2 b, \dots, A^{i-1}b \}, \quad i = 1,\dots, n.
\end{equation*}
These spaces are Krylov \emph{subspaces} because they are subspaces of $\RR^n$. The subspace $\calK_1$ is associated with a single basis vector $b$, and all subsequent subspaces are generated by adding to the basis a new vector that results from the multiplication of the last added vector by $A$.

We now wish to show three 



\begin{align*}
  \calK_i
  = \text{span}\{x_1,\dots,x_i\}
  = \text{span}\{ d_0,\dots,d_{i-1}\}
  = \text{span}\{r_0, \dots, r_{i-1} \}
\end{align*}



\section{Optimality of the conjugate gradient method}

%we introduce an \emph{iterative method} to solve $A x = b$ for $A$ sparse and SPD. Unlike a \emph{direct method} which yields the solution only at the termination of the method, an iterative method yields a series of increasingly accurate approximation to the solution $x$ of~\eqref{eq:cg_prob}. 
